import type { SupportedProvider } from "@shared";
import { expect, test } from "../fixtures";

// =============================================================================
// Test Configuration Interface
// =============================================================================

interface TokenCostLimitTestConfig {
  providerName: string;
  endpoint: (profileId: string) => string;
  headers: (wiremockStub: string) => Record<string, string>;
  buildRequest: (content: string) => object;
  modelName: string;
  tokenPrice: {
    provider: SupportedProvider;
    model: string;
    pricePerMillionInput: string;
    pricePerMillionOutput: string;
  };
}

// =============================================================================
// Test Configurations
// =============================================================================

const openaiConfig: TokenCostLimitTestConfig = {
  providerName: "OpenAI",

  endpoint: (profileId) => `/v1/openai/${profileId}/chat/completions`,

  headers: (wiremockStub) => ({
    Authorization: `Bearer ${wiremockStub}`,
    "Content-Type": "application/json",
  }),

  buildRequest: (content) => ({
    model: "test-gpt-4-cost-limit",
    messages: [{ role: "user", content }],
  }),

  modelName: "test-gpt-4-cost-limit",

  // WireMock returns: prompt_tokens: 100, completion_tokens: 20
  // Cost = (100 * 20000 + 20 * 30000) / 1,000,000 = $2.60
  tokenPrice: {
    provider: "openai",
    model: "test-gpt-4-cost-limit",
    pricePerMillionInput: "20000.00",
    pricePerMillionOutput: "30000.00",
  },
};

const anthropicConfig: TokenCostLimitTestConfig = {
  providerName: "Anthropic",

  endpoint: (profileId) => `/v1/anthropic/${profileId}/v1/messages`,

  headers: (wiremockStub) => ({
    "x-api-key": wiremockStub,
    "Content-Type": "application/json",
    "anthropic-version": "2023-06-01",
  }),

  buildRequest: (content) => ({
    model: "test-claude-cost-limit",
    max_tokens: 1024,
    messages: [{ role: "user", content }],
  }),

  modelName: "test-claude-cost-limit",

  // WireMock returns: input_tokens: 100, output_tokens: 20
  // Cost = (100 * 20000 + 20 * 30000) / 1,000,000 = $2.60
  tokenPrice: {
    provider: "anthropic",
    model: "test-claude-cost-limit",
    pricePerMillionInput: "20000.00",
    pricePerMillionOutput: "30000.00",
  },
};

const geminiConfig: TokenCostLimitTestConfig = {
  providerName: "Gemini",

  endpoint: (profileId) =>
    `/v1/gemini/${profileId}/v1beta/models/test-gemini-cost-limit:generateContent`,

  headers: (wiremockStub) => ({
    "x-goog-api-key": wiremockStub,
    "Content-Type": "application/json",
  }),

  buildRequest: (content) => ({
    contents: [
      {
        role: "user",
        parts: [{ text: content }],
      },
    ],
  }),

  modelName: "test-gemini-cost-limit",

  // WireMock returns: promptTokenCount: 100, candidatesTokenCount: 20
  // Cost = (100 * 20000 + 20 * 30000) / 1,000,000 = $2.60
  tokenPrice: {
    provider: "gemini",
    model: "test-gemini-cost-limit",
    pricePerMillionInput: "20000.00",
    pricePerMillionOutput: "30000.00",
  },
};

const cerebrasConfig: TokenCostLimitTestConfig = {
  providerName: "Cerebras",

  endpoint: (profileId) => `/v1/cerebras/${profileId}/chat/completions`,

  headers: (wiremockStub) => ({
    Authorization: `Bearer ${wiremockStub}`,
    "Content-Type": "application/json",
  }),

  buildRequest: (content) => ({
    model: "test-cerebras-cost-limit",
    messages: [{ role: "user", content }],
  }),

  modelName: "test-cerebras-cost-limit",

  // WireMock returns: prompt_tokens: 100, completion_tokens: 20
  // Cost = (100 * 20000 + 20 * 30000) / 1,000,000 = $2.60
  tokenPrice: {
    provider: "cerebras",
    model: "test-cerebras-cost-limit",
    pricePerMillionInput: "20000.00",
    pricePerMillionOutput: "30000.00",
  },
};

const mistralConfig: TokenCostLimitTestConfig = {
  providerName: "Mistral",

  endpoint: (profileId) => `/v1/mistral/${profileId}/chat/completions`,

  headers: (wiremockStub) => ({
    Authorization: `Bearer ${wiremockStub}`,
    "Content-Type": "application/json",
  }),

  buildRequest: (content) => ({
    model: "test-mistral-cost-limit",
    messages: [{ role: "user", content }],
  }),

  modelName: "test-mistral-cost-limit",

  // WireMock returns: prompt_tokens: 100, completion_tokens: 20
  // Cost = (100 * 20000 + 20 * 30000) / 1,000,000 = $2.60
  tokenPrice: {
    provider: "mistral",
    model: "test-mistral-cost-limit",
    pricePerMillionInput: "20000.00",
    pricePerMillionOutput: "30000.00",
  },
};

const vllmConfig: TokenCostLimitTestConfig = {
  providerName: "vLLM",

  endpoint: (profileId) => `/v1/vllm/${profileId}/chat/completions`,

  headers: (wiremockStub) => ({
    Authorization: `Bearer ${wiremockStub}`,
    "Content-Type": "application/json",
  }),

  buildRequest: (content) => ({
    model: "test-vllm-cost-limit",
    messages: [{ role: "user", content }],
  }),

  modelName: "test-vllm-cost-limit",

  // WireMock returns: prompt_tokens: 100, completion_tokens: 20
  // Cost = (100 * 20000 + 20 * 30000) / 1,000,000 = $2.60
  tokenPrice: {
    provider: "vllm",
    model: "test-vllm-cost-limit",
    pricePerMillionInput: "20000.00",
    pricePerMillionOutput: "30000.00",
  },
};

const ollamaConfig: TokenCostLimitTestConfig = {
  providerName: "Ollama",

  endpoint: (profileId) => `/v1/ollama/${profileId}/chat/completions`,

  headers: (wiremockStub) => ({
    Authorization: `Bearer ${wiremockStub}`,
    "Content-Type": "application/json",
  }),

  buildRequest: (content) => ({
    model: "test-ollama-cost-limit",
    messages: [{ role: "user", content }],
  }),

  modelName: "test-ollama-cost-limit",

  // WireMock returns: prompt_tokens: 100, completion_tokens: 20
  // Cost = (100 * 20000 + 20 * 30000) / 1,000,000 = $2.60
  tokenPrice: {
    provider: "ollama",
    model: "test-ollama-cost-limit",
    pricePerMillionInput: "20000.00",
    pricePerMillionOutput: "30000.00",
  },
};

const zhipuaiConfig: TokenCostLimitTestConfig = {
  providerName: "Zhipuai",

  endpoint: (profileId) => `/v1/zhipuai/${profileId}/chat/completions`,

  headers: (wiremockStub) => ({
    Authorization: `Bearer ${wiremockStub}`,
    "Content-Type": "application/json",
  }),

  buildRequest: (content) => ({
    model: "test-zhipuai-cost-limit",
    messages: [{ role: "user", content }],
  }),

  modelName: "test-zhipuai-cost-limit",

  // WireMock returns: prompt_tokens: 100, completion_tokens: 20
  // Cost = (100 * 20000 + 20 * 30000) / 1,000,000 = $2.60
  tokenPrice: {
    provider: "zhipuai",
    model: "test-zhipuai-cost-limit",
    pricePerMillionInput: "20000.00",
    pricePerMillionOutput: "30000.00",
  },
};

const cohereConfig: TokenCostLimitTestConfig = {
  providerName: "Cohere",

  endpoint: (profileId) => `/v1/cohere/${profileId}/chat`,

  headers: (wiremockStub) => ({
    Authorization: `Bearer ${wiremockStub}`,
    "Content-Type": "application/json",
  }),

  buildRequest: (content) => ({
    model: "test-cohere-cost-limit",
    messages: [{ role: "user", content: [{ type: "text", text: content }] }],
  }),

  modelName: "test-cohere-cost-limit",

  // WireMock returns: input_tokens: 100, output_tokens: 20
  // Cost = (100 * 20000 + 20 * 30000) / 1,000,000 = $2.60
  tokenPrice: {
    provider: "cohere",
    model: "test-cohere-cost-limit",
    pricePerMillionInput: "20000.00",
    pricePerMillionOutput: "30000.00",
  },
};

// =============================================================================
// Test Suite
// =============================================================================

const testConfigs: TokenCostLimitTestConfig[] = [
  openaiConfig,
  anthropicConfig,
  geminiConfig,
  cohereConfig,
  cerebrasConfig,
  mistralConfig,
  vllmConfig,
  ollamaConfig,
  zhipuaiConfig,
];

for (const config of testConfigs) {
  test.describe(`LLMProxy-TokenCostLimits-${config.providerName}`, () => {
    let profileId: string;
    let limitId: string;
    let tokenPriceId: string;

    const wiremockStub = `${config.providerName.toLowerCase()}-token-cost-limit-test`;

    test("blocks request when profile token cost limit is exceeded", async ({
      request,
      createAgent,
      createLimit,
      createTokenPrice,
      makeApiRequest,
      deleteTokenPrice,
      getTokenPrices,
    }) => {
      // 0. Delete any existing token prices for this model and create fresh ones
      const allPricesResponse = await getTokenPrices(request);
      if (allPricesResponse.ok()) {
        const allPrices = await allPricesResponse.json();
        const existingPrice = allPrices.find(
          (p: { provider: string; model: string; id: string }) =>
            p.provider === config.tokenPrice.provider &&
            p.model === config.tokenPrice.model,
        );
        if (existingPrice) {
          await deleteTokenPrice(request, existingPrice.id).catch(() => {});
        }
      }

      // Create fresh token price with exact values for our test
      const tokenPriceResponse = await createTokenPrice(
        request,
        config.tokenPrice,
      );
      const tokenPrice = await tokenPriceResponse.json();
      tokenPriceId = tokenPrice.id;

      // 1. Create a test profile
      const createResponse = await createAgent(
        request,
        `${config.providerName} Token Limit Test Profile`,
      );
      const profile = await createResponse.json();
      profileId = profile.id;

      // 2. Create profile-level limit with $2 value (each request costs $2.60, so usage exceeds limit after 1st request)
      // The limit check blocks when currentUsage >= limitValue, so with $2.60 usage after first request,
      // the second request will be blocked because $2.60 >= $2
      const limitResponse = await createLimit(request, {
        entityType: "agent",
        entityId: profileId,
        limitType: "token_cost",
        limitValue: 2,
        model: [config.modelName],
      });
      const limit = await limitResponse.json();
      limitId = limit.id;

      // 3. Make first request to set up usage (with long content to bypass optimization rules)
      const longContent =
        "This is a very long message to bypass optimization rules that typically only apply to short content under 1000 tokens. ".repeat(
          100,
        );

      const initialResponse = await makeApiRequest({
        request,
        method: "post",
        urlSuffix: config.endpoint(profileId),
        headers: config.headers(wiremockStub),
        data: config.buildRequest(longContent),
      });

      if (!initialResponse.ok()) {
        const errorText = await initialResponse.text();
        throw new Error(
          `Initial ${config.providerName} request failed: ${initialResponse.status()} ${errorText}`,
        );
      }

      // Poll for async usage tracking to complete
      // Usage tracking happens asynchronously after the response is sent
      // We need to wait until the usage is actually recorded before the second request
      // The limits endpoint returns modelUsage array with { model, tokensIn, tokensOut, cost }
      const maxPollingAttempts = 30;
      const pollingIntervalMs = 500;
      let usageTracked = false;

      for (let attempt = 0; attempt < maxPollingAttempts; attempt++) {
        const limitsResponse = await makeApiRequest({
          request,
          method: "get",
          urlSuffix: `/api/limits?entityType=agent&entityId=${profileId}`,
          ignoreStatusCheck: true,
        });

        if (limitsResponse.ok()) {
          const limits = await limitsResponse.json();
          const targetLimit = limits.find(
            (l: {
              id: string;
              modelUsage?: Array<{ model: string; cost: number }>;
            }) => l.id === limitId,
          );
          // Check if any model has recorded usage (cost > 0)
          const totalCost =
            targetLimit?.modelUsage?.reduce(
              (sum: number, m: { cost: number }) => sum + m.cost,
              0,
            ) ?? 0;
          if (totalCost > 0) {
            usageTracked = true;
            break;
          }
        }

        await new Promise((resolve) => setTimeout(resolve, pollingIntervalMs));
      }

      if (!usageTracked) {
        throw new Error(
          `Usage was not tracked after ${maxPollingAttempts * pollingIntervalMs}ms`,
        );
      }

      // 4. Second request should be blocked (limit exceeded)
      const blockedResponse = await makeApiRequest({
        request,
        method: "post",
        urlSuffix: config.endpoint(profileId),
        headers: config.headers(wiremockStub),
        data: config.buildRequest(
          "This should be blocked because we exceeded the limit",
        ),
        ignoreStatusCheck: true,
      });

      // 5. Verify 429 response with token_cost_limit_exceeded code
      expect(blockedResponse.status()).toBe(429);
      const errorBody = await blockedResponse.json();
      expect(errorBody.error.code).toBe("token_cost_limit_exceeded");
      expect(errorBody.error.type).toBe("rate_limit_exceeded");
    });

    test("allows request when under limit", async ({
      request,
      createAgent,
      createLimit,
      createTokenPrice,
      makeApiRequest,
    }) => {
      // 0. Create token price for the model
      const tokenPriceResponse = await createTokenPrice(
        request,
        config.tokenPrice,
      );
      if (tokenPriceResponse.ok()) {
        const tokenPrice = await tokenPriceResponse.json();
        tokenPriceId = tokenPrice.id;
      }

      // 1. Create a test profile
      const createResponse = await createAgent(
        request,
        `${config.providerName} Token Limit OK Test Profile`,
      );
      const profile = await createResponse.json();
      profileId = profile.id;

      // 2. Create profile-level limit with high value
      const limitResponse = await createLimit(request, {
        entityType: "agent",
        entityId: profileId,
        limitType: "token_cost",
        limitValue: 1000,
        model: [config.modelName],
      });
      const limit = await limitResponse.json();
      limitId = limit.id;

      // 3. First request should succeed
      const response1 = await makeApiRequest({
        request,
        method: "post",
        urlSuffix: config.endpoint(profileId),
        headers: config.headers(wiremockStub),
        data: config.buildRequest("Hello"),
      });
      expect(response1.ok()).toBeTruthy();

      // 4. Second request should also succeed (still under limit)
      const response2 = await makeApiRequest({
        request,
        method: "post",
        urlSuffix: config.endpoint(profileId),
        headers: config.headers(wiremockStub),
        data: config.buildRequest("Hello again"),
      });
      expect(response2.ok()).toBeTruthy();
    });

    test.afterEach(
      async ({ request, deleteLimit, deleteAgent, deleteTokenPrice }) => {
        if (limitId) {
          await deleteLimit(request, limitId).catch(() => {});
          limitId = "";
        }
        if (profileId) {
          await deleteAgent(request, profileId).catch(() => {});
          profileId = "";
        }
        if (tokenPriceId) {
          await deleteTokenPrice(request, tokenPriceId).catch(() => {});
          tokenPriceId = "";
        }
      },
    );
  });
}
