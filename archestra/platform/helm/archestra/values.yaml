archestra:
  # The Docker image to use for the Archestra Platform
  # This image contains both the backend API and frontend
  image: archestra/platform

  # The image tag for the Archestra Platform
  #
  # This value is automatically updated by release-please during releases.
  # We use the "generic" updater with inline annotation to preserve YAML comments throughout the file.
  # See: https://github.com/googleapis/release-please/blob/main/docs/customizing.md#updating-arbitrary-files
  # See: https://github.com/googleapis/release-please/issues/2195
  imageTag: "1.0.44" # x-release-please-version

  # Image pull policy for the Archestra container
  # Options: Always, IfNotPresent, Never
  imagePullPolicy: IfNotPresent

  # Number of pod replicas for the Archestra Platform deployment
  # Note: This value is ignored when horizontalPodAutoscaler.enabled is true,
  # as the HPA will manage the replica count dynamically
  replicaCount: 1

  # Annotations to add to pods created by the Deployment
  # This is useful for integrations that require pod-level annotations, such as:
  # - Service mesh sidecars (Istio, Linkerd)
  # - Prometheus scraping
  # - Cloud provider features (AWS IAM roles, GKE Workload Identity)
  # - Secrets injection (Vault agent, External Secrets Operator)
  # Example:
  #   podAnnotations:
  #     prometheus.io/scrape: "true"
  #     prometheus.io/port: "9050"
  #     vault.hashicorp.com/agent-inject: "true"
  #     sidecar.istio.io/inject: "true"
  podAnnotations: {}

  # Node selector for scheduling pods on specific nodes
  # This is useful for targeting specific node pools or nodes with particular labels
  # See: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
  # Example:
  #   nodeSelector:
  #     karpenter.sh/nodepool: general-purpose
  #     kubernetes.io/os: linux
  #     node.kubernetes.io/instance-type: m5.large
  nodeSelector: {}

  # Deployment strategy configuration for the Deployment
  # See https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/deployment-v1/#:~:text=strategy%20(DeploymentStrategy)
  # Default is RollingUpdate with maxUnavailable: 0 to ensure zero-downtime deployments.
  # This ensures at least one pod is always available during updates, which is critical
  # for GKE ingress health checks to always have healthy endpoints.
  deploymentStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1

  # Additional environment variables to pass to the container
  # These will be merged with the default DATABASE_URL environment variable
  #
  # Supports Kubernetes environment variable expansion using $(VAR_NAME) syntax.
  # This allows referencing other environment variables defined in the container.
  # See: https://kubernetes.io/docs/tasks/inject-data-application/define-interdependent-environment-variables/
  #
  # Example:
  #   env:
  #     # External URL(s) for connection instructions (supports comma-separated list for multiple URLs)
  #     ARCHESTRA_API_BASE_URL: "http://archestra.default.svc:9000,https://api.archestra.example.com"
  #     CUSTOM_VAR: "custom-value"
  #     # Reference another env var (defined via envWithValueFrom below)
  #     ARCHESTRA_OTEL_EXPORTER_OTLP_ENDPOINT: "http://$(NODE_IP):4317"
  env: {}

  # Environment variables with valueFrom (for fieldRef, resourceFieldRef, configMapKeyRef)
  # Use this for Kubernetes downward API or other valueFrom sources
  # This is required if you want to use $(VAR_NAME) expansion referencing pod/node information
  # See: https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/
  #
  # Example - expose node IP for OTEL collector endpoint:
  #   envWithValueFrom:
  #     - name: NODE_IP
  #       valueFrom:
  #         fieldRef:
  #           fieldPath: status.hostIP
  #     - name: POD_NAME
  #       valueFrom:
  #         fieldRef:
  #           fieldPath: metadata.name
  #     - name: POD_NAMESPACE
  #       valueFrom:
  #         fieldRef:
  #           fieldPath: metadata.namespace
  #     - name: MEMORY_LIMIT
  #       valueFrom:
  #         resourceFieldRef:
  #           containerName: archestra-platform
  #           resource: limits.memory
  envWithValueFrom: []

  # Environment variables from Kubernetes Secrets
  # This allows you to inject sensitive data from secrets into the container
  # See: https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/#define-container-environment-variables-using-secret-data
  # Example:
  #   envFromSecrets:
  #     - name: API_KEY
  #       secretName: my-api-credentials
  #       secretKey: api-key
  #     - name: DATABASE_PASSWORD
  #       secretName: db-credentials
  #       secretKey: password
  envFromSecrets: []

  # Import all key-value pairs from Secrets or ConfigMaps as environment variables
  # This is useful when you want to inject all keys from a Secret/ConfigMap without specifying each one individually
  # See: https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/#configure-all-key-value-pairs-in-a-secret-as-container-environment-variables
  # Example:
  #   envFrom:
  #     - secretRef:
  #         name: my-secret
  #     - configMapRef:
  #         name: my-configmap
  #     - secretRef:
  #         name: another-secret
  #         optional: true
  envFrom: []

  # Init containers configuration
  initContainers:
    # Wait for PostgreSQL to be ready before starting the application
    # Disable this if your database is always available (e.g., managed cloud database)
    # or if you handle readiness checks in another way
    waitForPostgres:
      enabled: true

    # Vault secret injection init container
    # Similar to Vault Agent Injector (writes secrets to a shared volume) but:
    # - No extra infrastructure needed (no Vault Agent Injector deployment)
    # - Secrets are loaded as env vars, not files
    # Uses the existing ARCHESTRA_HASHICORP_VAULT_* env vars for connectivity and auth
    #
    # Example:
    #   vaultSecrets:
    #     enabled: true
    #     secrets:
    #       - envVar: DATABASE_URL
    #         path: secret/data/team/database_url
    #         key: url
    #       - envVar: MS_TEAMS_SECRET
    #         path: secret/data/team/ms_teams_secret
    #         key: value
    vaultSecrets:
      enabled: false
      # Container entrypoint to exec after loading secrets (must match Dockerfile ENTRYPOINT)
      entrypoint: /docker-entrypoint.sh
      # Each entry: envVar (env var name), path (Vault API path after /v1/), key (key within secret data)
      secrets: []

  # Resource requests and limits for the Archestra container
  # See: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  resources:
    requests:
      memory: "2Gi"
    limits:
      memory: "3Gi"

  # Configuration for the MCP Server Runtime
  orchestrator:
    # Base image for MCP server containers
    # If not set, defaults to "europe-west1-docker.pkg.dev/friendly-path-465518-r6/archestra-public/mcp-server-base:latest"
    baseImage: ""

    # Kubernetes configuration for MCP Server Runtime
    # This controls how the Archestra platform connects to Kubernetes to manage MCP server pods
    kubernetes:
      # Namespace where MCP server pods will be created
      # If not set, defaults to the namespace specified in the helm release
      namespace: ""

      # Kubeconfig configuration
      # Choose one of the following methods:

      # Method 1: Use in-cluster configuration (recommended when running inside K8s)
      # No additional configuration needed - the platform will automatically use the service account
      loadKubeconfigFromCurrentCluster: true

      # Method 2: Mount a kubeconfig file from a secret
      # Set useInClusterConfig to false and configure the secret below
      kubeconfig:
        # Enable kubeconfig volume mount
        enabled: false

        # Name of the secret containing the kubeconfig file
        # The secret should have a key named "config" containing the kubeconfig content
        # Example of creating the secret:
        #   kubectl create secret generic archestra-kubeconfig --from-file=config=/path/to/kubeconfig
        secretName: ""

        # Path where the kubeconfig will be mounted inside the container
        # This will be set as the KUBECONFIG environment variable
        mountPath: /etc/kubeconfig

      # ServiceAccount configuration for the Archestra Platform
      serviceAccount:
        # Specifies whether a service account should be created
        create: true
        # Annotations to add to the service account
        # Use this for cloud provider integrations like GKE Workload Identity or AWS IRSA
        #
        # GKE Workload Identity example (for Vertex AI authentication):
        #   annotations:
        #     iam.gke.io/gcp-service-account: my-gsa@my-project.iam.gserviceaccount.com
        #
        # AWS IRSA example:
        #   annotations:
        #     eks.amazonaws.com/role-arn: arn:aws:iam::111122223333:role/my-role
        annotations: {}
        # The name of the service account to use.
        # If not set and create is true, a name is generated using the fullname template
        name: ""
        # Image pull secrets to attach to the service account (automatically used by pods)
        # This is the recommended way to configure image pull secrets
        # Example:
        #   imagePullSecrets:
        #     - name: myregistrykey
        imagePullSecrets: []

      # RBAC (Role-Based Access Control) configuration
      rbac:
        # Specifies whether RBAC resources should be created
        create: true

      # RBAC configuration for MCP server pods that need Kubernetes API access
      # Creates a dedicated service account with namespace-scoped permissions for K8s resources
      # Permissions: manage pods, services, deployments, jobs (read-only for secrets removed)
      # IMPORTANT: Only the Kubernetes MCP server uses this service account - all other MCP servers
      # use the default service account with no permissions, maintaining proper isolation
      mcpServerRbac:
        # Specifies whether RBAC resources (ServiceAccount, Role, RoleBinding) should be created
        create: true

        # Additional ClusterRoleBindings to attach to the mcp-k8s-operator service account
        # Use this to grant cluster-wide permissions (e.g., read access to all namespaces)
        # The ClusterRoles must be created externally (e.g., via Terraform or kubectl)
        # Example:
        #   additionalClusterRoleBindings:
        #     - clusterRoleName: cluster-reader
        #     - clusterRoleName: custom-role
        #       name: custom-binding-name  # Optional: override the auto-generated binding name
        additionalClusterRoleBindings: []

        # Additional RoleBindings to attach to the mcp-k8s-operator service account
        # Use this to grant namespace-scoped permissions (e.g., full access to specific namespaces)
        # The Roles must be created externally (e.g., via Terraform or kubectl) in the specified namespaces
        # Example:
        #   additionalRoleBindings:
        #     - roleName: my-namespace-admin
        #       namespace: my-app-namespace
        #     - roleName: another-role
        #       namespace: another-namespace
        #       name: custom-binding-name  # Optional: override the auto-generated binding name
        additionalRoleBindings: []

  # Service configuration
  service:
    # Service type - ClusterIP, NodePort, or LoadBalancer
    type: ClusterIP
    # Annotations to add to the Kubernetes Service
    annotations: {}
    # Node ports for NodePort service type (optional)
    # Only used when service.type is NodePort
    nodePorts:
      backend: null
      metrics: null
      frontend: null

  # Ingress configuration
  ingress:
    # Enable or disable ingress creation
    enabled: false

    # Annotations for the ingress resource
    annotations: {}

    # Complete ingress specification
    spec: {}

  # HorizontalPodAutoscaler configuration
  # Automatically scales the number of pod replicas based on observed metrics
  # See: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  horizontalPodAutoscaler:
    # Enable or disable HorizontalPodAutoscaler creation
    enabled: false

    # Minimum number of replicas the autoscaler can scale down to
    minReplicas: 1

    # Maximum number of replicas the autoscaler can scale up to
    maxReplicas: 10

    # Metrics used to determine scaling behavior
    # Each metric can be of type: Resource, Pods, Object, External, or ContainerResource
    # Example with CPU utilization:
    #   metrics:
    #     - type: Resource
    #       resource:
    #         name: cpu
    #         target:
    #           type: Utilization
    #           averageUtilization: 80
    metrics: []

    # Scaling behavior configuration for scale up and scale down
    # Allows fine-grained control over scaling speed and stabilization
    # Example:
    #   behavior:
    #     scaleDown:
    #       stabilizationWindowSeconds: 300
    #       policies:
    #         - type: Percent
    #           value: 10
    #           periodSeconds: 60
    #     scaleUp:
    #       stabilizationWindowSeconds: 0
    #       policies:
    #         - type: Percent
    #           value: 100
    #           periodSeconds: 15
    behavior: {}

  # PodDisruptionBudget configuration
  # Limits the number of pods that can be down simultaneously during voluntary disruptions
  # See: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
  podDisruptionBudget:
    # Enable or disable PodDisruptionBudget creation
    enabled: false

    # Minimum number of pods that must remain available during a disruption
    # Can be an integer (e.g., 1) or a percentage string (e.g., "50%")
    # Only one of minAvailable or maxUnavailable can be set
    minAvailable: null

    # Maximum number of pods that can be unavailable during a disruption
    # Can be an integer (e.g., 1) or a percentage string (e.g., "25%")
    # Only one of minAvailable or maxUnavailable can be set
    maxUnavailable: null

    # Policy for evicting unhealthy pods
    # Options: IfHealthyBudget, AlwaysAllow
    # - IfHealthyBudget: Running pods that are not yet healthy can be evicted only if the guarded application is not disrupted
    # - AlwaysAllow: Running pods that are not yet healthy can be evicted regardless of whether the criteria in a PodDisruptionBudget is met
    # See: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy
    unhealthyPodEvictionPolicy: null

  # GKE BackendConfig configuration
  # Creates GKE-specific BackendConfig resources for configuring Google Cloud Load Balancer behavior
  # See: https://cloud.google.com/kubernetes-engine/docs/how-to/ingress-configuration#configuring_backendconfig
  #
  # NOTE: These health check settings are tuned to work with Kubernetes startup probes that allow
  # up to 305 seconds (30 * 10s + 5s) for pods to start.
  # unhealthyThreshold=10 with checkIntervalSec=15 allows ~150 seconds before marking unhealthy,
  # giving pods enough time to start during deployments while still detecting failures.
  gkeBackendConfig:
    # Enable or disable GKE BackendConfig resources
    # When enabled, creates two BackendConfig resources: one for the backend (port 9000) and one for the frontend (port 3000)
    enabled: false

    # Backend API configuration (port 9000)
    backend:
      # Request timeout in seconds for the load balancer
      # Recommended: 600 (10 minutes) for streaming responses and long-running MCP operations
      # If not set, uses GKE default (30 seconds)
      timeoutSec: null

      # Connection draining configuration
      # connectionDraining:
      #   drainingTimeoutSec: 60

      # Health check configuration for the backend service
      healthCheck:
        checkIntervalSec: 15
        timeoutSec: 10
        healthyThreshold: 1
        unhealthyThreshold: 10
        type: HTTP
        requestPath: /health
        port: 9000

    # Frontend configuration (port 3000)
    frontend:
      # Request timeout in seconds for the load balancer
      # If not set, uses GKE default (30 seconds)
      timeoutSec: null

      # Connection draining configuration
      # connectionDraining:
      #   drainingTimeoutSec: 60

      # Health check configuration for the frontend service
      healthCheck:
        checkIntervalSec: 15
        timeoutSec: 10
        healthyThreshold: 1
        unhealthyThreshold: 10
        type: HTTP
        requestPath: /health
        port: 3000

# PostgreSQL configuration
postgresql:
  # External database URL to use instead of deploying a PostgreSQL instance
  # If null, a PostgreSQL instance will be deployed using the Bitnami PostgreSQL chart
  # Accepts both postgres:// and postgresql:// URI schemes
  #
  # Option 1 - Set URL here (stored in K8s secret automatically):
  #   external_database_url: postgres://username:password@host:5432/database
  #
  # Option 2 - Use pre-existing K8s secret (leave external_database_url as null):
  #   archestra:
  #     envFromSecrets:
  #       - name: ARCHESTRA_DATABASE_URL
  #         secretName: my-db-secret
  #         secretKey: database-url
  #
  # Option 3 - Database URL from Vault init container (set to "vault"):
  #   external_database_url: "from_vault"
  #   This skips ARCHESTRA_DATABASE_URL secret creation and instead relies on the vault-secrets init container
  #   to provide the database URL at runtime.
  #
  # IMPORTANT: When using external database, you MUST also set enabled=false
  external_database_url: null

  # Whether to deploy the bundled PostgreSQL instance
  # Set to false when using external_database_url
  enabled: true
  auth:
    database: archestra_dev
    username: archestra
    # Password is auto-generated by the Bitnami PostgreSQL chart and stored in a Kubernetes Secret.
    # The password is persisted across helm upgrades (Bitnami uses lookup to preserve existing secrets).
    # The secret is named <release>-archestra-platform-postgresql with key "password".
    # To use a custom password, uncomment and set the password below:
    # password: "your-custom-password"
  # Resource configuration for PostgreSQL
  # See: https://artifacthub.io/packages/helm/bitnami/postgresql for all options
  primary:
    resources:
      requests:
        cpu: "1000m"
        memory: "1Gi"
      limits:
        cpu: "1000m"
        memory: "1Gi"
  # NOTE: Bitnami is archiving their image.. see these github comments for details
  # (this is essentially why we need to override image.repository and image.tag and global.security.allowInsecureImages)
  #
  # https://github.com/coder/coder/issues/19869#issuecomment-3305875979
  # https://github.com/bitnami/containers/issues/83267
  image:
    repository: bitnamisecure/postgresql
    tag: latest
  global:
    security:
      allowInsecureImages: true
